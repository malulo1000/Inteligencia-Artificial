{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "h-lvxkvxBBue",
        "zA-rlMdlBIa1",
        "1kQXGR4MBK9H",
        "MyNxtotBBOb1",
        "ZcoZ2NfaIuYD",
        "VoOu_dU47o7r",
        "ZqZLwOG-AcMv",
        "su696zhJTjWE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2033296dfa75434880631faf63569ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34e7b9aaa2c8434b8b8f24dfceec4f78",
              "IPY_MODEL_afb7cee762dc458bb96d3be053654166",
              "IPY_MODEL_f6243b8350d644d586bd6844ac44bbf7"
            ],
            "layout": "IPY_MODEL_5c538c64e8b14506ad87fadaacbdb226"
          }
        },
        "34e7b9aaa2c8434b8b8f24dfceec4f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e947f061dc64c4e985411a84ee95ebd",
            "placeholder": "​",
            "style": "IPY_MODEL_80578222517447deab92be6073cba5d6",
            "value": "model.safetensors: 100%"
          }
        },
        "afb7cee762dc458bb96d3be053654166": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c09b314cf3184d37b075714c8d9e78be",
            "max": 1421700479,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fcbaf07bc9e4519afea4124d2bd7c48",
            "value": 1421700479
          }
        },
        "f6243b8350d644d586bd6844ac44bbf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6cfa6e1a98049e8ace25223a684e350",
            "placeholder": "​",
            "style": "IPY_MODEL_4bde0fd8d8fe4c178e7c09c00ef3ac8d",
            "value": " 1.42G/1.42G [00:24&lt;00:00, 46.8MB/s]"
          }
        },
        "5c538c64e8b14506ad87fadaacbdb226": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e947f061dc64c4e985411a84ee95ebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80578222517447deab92be6073cba5d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c09b314cf3184d37b075714c8d9e78be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fcbaf07bc9e4519afea4124d2bd7c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6cfa6e1a98049e8ace25223a684e350": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bde0fd8d8fe4c178e7c09c00ef3ac8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multichoice Question Answering\n",
        "\n",
        "\n",
        "\n",
        "*   Elian Paniagua\n",
        "*   Luciana Huertas\n",
        "*   Sebastian Linares\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3XiHTQR7XRfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm -q"
      ],
      "metadata": {
        "id": "0xP5TZLNLSlK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importación de librerías y carga de datos"
      ],
      "metadata": {
        "id": "h-lvxkvxBBue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import RobertaTokenizer, RobertaForMultipleChoice\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Cargamos el archivo CSV\n",
        "file_path = 'train.csv'\n",
        "df = pd.read_csv(file_path, sep='\\t')\n",
        "\n",
        "# Dividimos el conjunto de datos en entrenamiento y validación\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Inicializamos el tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')"
      ],
      "metadata": {
        "id": "bkZ7A1NgeBzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición del Dataset Personalizado"
      ],
      "metadata": {
        "id": "zA-rlMdlBIa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajusta dinámicamente los datos por batch,\n",
        "# necesario para crear tensores del mismo tamaño en cada batch.\n",
        "def collate_fn(batch):\n",
        "    max_len = max([item['input_ids'].size(1) for item in batch])\n",
        "\n",
        "    input_ids = torch.stack([torch.cat([item['input_ids'], torch.full((item['input_ids'].size(0), max_len - item['input_ids'].size(1)), tokenizer.pad_token_id, dtype=torch.long)], dim=1) for item in batch])\n",
        "    attention_mask = torch.stack([torch.cat([item['attention_mask'], torch.full((item['attention_mask'].size(0), max_len - item['attention_mask'].size(1)), 0, dtype=torch.long)], dim=1) for item in batch])\n",
        "    labels = torch.tensor([item['labels'] for item in batch], dtype=torch.long)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "# Define el dataset personalizado para preguntas de opción múltiple.\n",
        "class CustomRaceAnsweringModel(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.label_mapping = {label: i for i, label in enumerate([\"A\", \"B\", \"C\", \"D\", \"E\"])}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.data.iloc[idx]\n",
        "        context = example['text'] or \"\"\n",
        "        question = example['question'] or \"\"\n",
        "        options = [example['A'], example['B'], example['C'], example['D'], example['E']]\n",
        "        reason = example['reason'] or \"\"\n",
        "        label = self.label_mapping[example['answer']]\n",
        "\n",
        "        # Preprocesamiento de texto\n",
        "        context = context[:self.max_length // 3]\n",
        "        question = question[:self.max_length // 6]\n",
        "        reason = reason[:self.max_length // 6]\n",
        "\n",
        "        # Tokenización de las opciones\n",
        "        c_plus_q_r = f\"{context} {tokenizer.bos_token} {question} {tokenizer.sep_token} {reason} {tokenizer.sep_token}\"\n",
        "        c_plus_q_r_5 = [c_plus_q_r + f\" {option}\" for option in options]\n",
        "\n",
        "        tokenized_examples = self.tokenizer(\n",
        "            c_plus_q_r_5,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"longest\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        input_ids = tokenized_examples['input_ids']\n",
        "        attention_mask = tokenized_examples['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Creamos los datasets\n",
        "train_dataset = CustomRaceAnsweringModel(train_df, tokenizer)\n",
        "val_dataset = CustomRaceAnsweringModel(val_df, tokenizer)\n",
        "\n",
        "# Creamos los dataloaders para cargar los datos en batches de tamaño 4\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, pin_memory=True, num_workers=2, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, pin_memory=True, num_workers=2, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "MHrWnB4EIlC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicialización del modelo y optimizador"
      ],
      "metadata": {
        "id": "1kQXGR4MBK9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForMultipleChoice.from_pretrained('roberta-large')\n",
        "\n",
        "# Verificamos si se dispone de GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Definimos el optimizador\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "# Inicializamos el escalador para FP16\n",
        "scaler = GradScaler()"
      ],
      "metadata": {
        "id": "fMTWRCYGIpcH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "2033296dfa75434880631faf63569ae6",
            "34e7b9aaa2c8434b8b8f24dfceec4f78",
            "afb7cee762dc458bb96d3be053654166",
            "f6243b8350d644d586bd6844ac44bbf7",
            "5c538c64e8b14506ad87fadaacbdb226",
            "5e947f061dc64c4e985411a84ee95ebd",
            "80578222517447deab92be6073cba5d6",
            "c09b314cf3184d37b075714c8d9e78be",
            "6fcbaf07bc9e4519afea4124d2bd7c48",
            "e6cfa6e1a98049e8ace25223a684e350",
            "4bde0fd8d8fe4c178e7c09c00ef3ac8d"
          ]
        },
        "outputId": "a7727bd7-2ca7-4d7e-9710-aa63b482466a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2033296dfa75434880631faf63569ae6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones de Entrenamiento y Validación"
      ],
      "metadata": {
        "id": "MyNxtotBBOb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, optimizer, device, scaler, epochs=6, accumulation_steps=4):\n",
        "    model.train()\n",
        "    val_accuracies = []\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        print(f\"\\nIniciando epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        # Barra de progreso para monitorear el entrenamiento\n",
        "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        for i, batch in progress_bar:\n",
        "            # Mover los tensores a la GPU\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Usar autocast para precisión mixta (FP16)\n",
        "            with autocast(device_type='cuda'):\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * accumulation_steps\n",
        "            progress_bar.set_postfix({\"Training Loss\": total_loss / (i + 1)})\n",
        "\n",
        "        # Pérdida promedio por epoch\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} completado, Pérdida promedio: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validar el modelo en el conjunto de validación\n",
        "        val_accuracy = validate_model(model, val_loader, device)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} completado, Precisión de validación: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Graficar resultados\n",
        "    plot_metrics(train_losses, val_accuracies, epochs)\n",
        "\n",
        "def validate_model(model, val_loader, device):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Obtener las predicciones del modelo\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            # Guardar las etiquetas reales y las predicciones\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_predictions)\n",
        "    return acc\n",
        "\n",
        "# Graficar la pérdida y precisión\n",
        "def plot_metrics(train_losses, val_accuracies, epochs):\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, epochs + 1), train_losses, marker='o', color='blue')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Training Loss')\n",
        "    plt.title('Training Loss Over Epochs')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, epochs + 1), val_accuracies, marker='o', color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Validation Accuracy')\n",
        "    plt.title('Validation Accuracy Over Epochs')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "knJgips0Is53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración y ejecución del entrenamiento"
      ],
      "metadata": {
        "id": "ZcoZ2NfaIuYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"El entrenamiento se realizará en: {device}\")\n",
        "train_model(model, train_loader, val_loader, optimizer, device, scaler, epochs=6)\n",
        "print(\"Entrenamiento completado exitosamente.\")"
      ],
      "metadata": {
        "id": "D9d-G9QIIwXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11cf2c9-45fc-426f-8581-bf6e9b7cd609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El entrenamiento se realizará en: cuda\n",
            "Iniciando el entrenamiento...\n",
            "\n",
            "Iniciando epoch 1/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/8: 100%|██████████| 1406/1406 [12:17<00:00,  1.91it/s, Training Loss=1.46]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8 completado, Pérdida promedio: 1.4621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión de validación: 0.5669\n",
            "Epoch 1/8 completado, Precisión de validación: 0.5669\n",
            "\n",
            "Iniciando epoch 2/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/8: 100%|██████████| 1406/1406 [12:15<00:00,  1.91it/s, Training Loss=1.08]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/8 completado, Pérdida promedio: 1.0798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardar el modelo en Google Drive (Funciones y Configuración)\n",
        "\n",
        "Si vas a usar este campo no es necesario correr las celdas anteriores\n",
        "- Funciones ...\n",
        "- Personalización ..."
      ],
      "metadata": {
        "id": "VoOu_dU47o7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "# Montamos Google Drive para almacenar el modelo\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definimos la ruta en Google Drive donde se guardará el modelo\n",
        "model_save_path = '/content/drive/My Drive/Multichoice Question'\n",
        "\n",
        "# Creamos la carpeta si no existe\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# Guardar el mejor modelo basado en precisión de validación\n",
        "def save_best_model(model, val_accuracy, model_save_path, best_accuracy):\n",
        "    if val_accuracy > best_accuracy:\n",
        "        model_file = \"best_model.pt\"\n",
        "        save_path = os.path.join(model_save_path, model_file)\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"Nuevo mejor modelo guardado con precisión de validación: {val_accuracy:.4f}\")\n",
        "        return val_accuracy\n",
        "    return best_accuracy\n",
        "\n",
        "# Validar el modelo\n",
        "def validate_model(model, val_loader, device):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_predictions)\n",
        "    print(f\"Precisión de validación: {acc:.4f}\")\n",
        "    return acc\n",
        "\n",
        "# Entrenamiento con validación y guardado del mejor modelo\n",
        "def train_model(model, train_loader, val_loader, optimizer, device, scaler, epochs=6, accumulation_steps=4):\n",
        "    best_val_accuracy = 0.0\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        print(f\"\\nIniciando epoch {epoch + 1}/{epochs}\")\n",
        "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        for i, batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            with autocast(device_type='cuda'):\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * accumulation_steps\n",
        "            progress_bar.set_postfix({\"Training Loss\": total_loss / (i + 1)})\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} completado, Pérdida promedio: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validar el modelo y guardar si es el mejor\n",
        "        val_accuracy = validate_model(model, val_loader, device)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        best_val_accuracy = save_best_model(model, val_accuracy, model_save_path, best_val_accuracy)\n",
        "\n",
        "    plot_training_validation_metrics(train_losses, val_accuracies, epochs)\n",
        "\n",
        "# Graficar las métricas de entrenamiento y validación\n",
        "def plot_training_validation_metrics(train_losses, val_accuracies, epochs):\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, epochs + 1), train_losses, marker='o', color='blue')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Training Loss')\n",
        "    plt.title('Training Loss Over Epochs')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, epochs + 1), val_accuracies, marker='o', color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Validation Accuracy')\n",
        "    plt.title('Validation Accuracy Over Epochs')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1nVVTq8U7ot7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"El entrenamiento se realizará en: {device}\")\n",
        "model.to(device)\n",
        "\n",
        "# Inicializamos el escalador para FP16\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Entrenamos el modelo\n",
        "print(\"Iniciando el entrenamiento...\")\n",
        "train_model(model, train_loader, val_loader, optimizer, device, scaler, epochs=25)\n",
        "print(\"Entrenamiento completado exitosamente.\")"
      ],
      "metadata": {
        "id": "NrSdhF5Wxlpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TEST\n",
        "\n",
        "No es necesario correr ninguna celda anterior"
      ],
      "metadata": {
        "id": "ZqZLwOG-AcMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importación de librerías y configuración del dataloader"
      ],
      "metadata": {
        "id": "su696zhJTjWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaForMultipleChoice\n",
        "\n",
        "# Definición del collate_fn para manejar batches de datos\n",
        "def collate_fn(batch):\n",
        "    max_len = max([item['input_ids'].size(1) for item in batch])  # Longitud máxima del batch\n",
        "\n",
        "    input_ids = torch.stack([torch.cat([item['input_ids'], torch.full((item['input_ids'].size(0), max_len - item['input_ids'].size(1)), tokenizer.pad_token_id, dtype=torch.long)], dim=1) for item in batch])\n",
        "    attention_mask = torch.stack([torch.cat([item['attention_mask'], torch.full((item['attention_mask'].size(0), max_len - item['attention_mask'].size(1)), 0, dtype=torch.long)], dim=1) for item in batch])\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "    }\n",
        "\n",
        "# Definir la clase CustomRaceAnsweringModel para manejar el dataset de test\n",
        "class CustomRaceAnsweringModel(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data.reset_index(drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.data.iloc[idx]\n",
        "        context = example['text'] or \"\"\n",
        "        question = example['question'] or \"\"\n",
        "        options = [example['A'], example['B'], example['C'], example['D'], example['E']]\n",
        "\n",
        "        # Preprocesar y recortar el texto si es necesario\n",
        "        context = context[:self.max_length // 3]\n",
        "        question = question[:self.max_length // 6]\n",
        "\n",
        "        # Concatenar el contexto y la pregunta con las opciones\n",
        "        sep_token = self.tokenizer.sep_token or \"\"\n",
        "        bos_token = self.tokenizer.bos_token or \"\"\n",
        "        c_plus_q = f\"{context} {bos_token} {question} {sep_token}\"\n",
        "        c_plus_q_options = [c_plus_q + f\" {option}\" for option in options]\n",
        "\n",
        "        # Tokenización de las opciones\n",
        "        tokenized_examples = self.tokenizer(\n",
        "            c_plus_q_options,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"longest\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        input_ids = tokenized_examples['input_ids']\n",
        "        attention_mask = tokenized_examples['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n",
        "# Inicializar el tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
        "\n",
        "# Cargar el archivo CSV de test\n",
        "test_file_path = 'test.csv'  # Ruta donde subiste el archivo test.csv\n",
        "test_df = pd.read_csv(test_file_path, sep='\\t')\n",
        "\n",
        "# Crear el dataset para test.csv\n",
        "test_dataset = CustomRaceAnsweringModel(test_df, tokenizer)\n",
        "\n",
        "# Crear el dataloader\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, pin_memory=True, num_workers=2, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "aXXPTzvjAdjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6faaa78c-876f-4677-afad-ee52ac1580bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Montar Google Drive"
      ],
      "metadata": {
        "id": "LummylBWTre9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6HX-iTgLBz7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cbf49e2-a7d7-41ba-af4e-d3fd078cb9f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inicialización del modelo y carga de pesos entrenados"
      ],
      "metadata": {
        "id": "Vk0N4gmcT3qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForMultipleChoice.from_pretrained('roberta-large')\n",
        "\n",
        "# Verificamos si se dispone de GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Cargamos el modelo previamente entrenado\n",
        "model_save_path = '/content/drive/My Drive/Multichoice Question/best_model.pt'\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Xy6RNpSxAeuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34acedc-bac3-44e1-a1d0-76f8f217f623"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-3-670fff9637e0>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_save_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForMultipleChoice(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=1024, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicción sobre test.csv y generación de archivo con respuestas"
      ],
      "metadata": {
        "id": "eeXFKLuFT63b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_save(model, test_loader, device, output_file='test.txt'):\n",
        "    model.eval()\n",
        "    predictions_list = []\n",
        "    label_mapping = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\", 4: \"E\"}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            # Convertir las predicciones numéricas en etiquetas\n",
        "            predictions_list.extend([label_mapping[pred.item()] for pred in predictions])\n",
        "\n",
        "    # Guardar las predicciones en un archivo .txt\n",
        "    with open(output_file, 'w') as f:\n",
        "        for pred in predictions_list:\n",
        "            f.write(pred + '\\n')\n",
        "\n",
        "    print(f\"Predicciones guardadas en {output_file}\")\n",
        "\n",
        "# Ejecutamos la predicción y guardar el archivo\n",
        "predict_and_save(model, test_loader, device, output_file='test.txt')"
      ],
      "metadata": {
        "id": "DiwHRgKJd6kn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}